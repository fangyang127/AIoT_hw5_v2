import time
from pathlib import Path

import matplotlib.pyplot as plt
import streamlit as st

from models import get_available_hf_models, load_baseline_model, load_hf_model, predict_proba
from utils import (
    PREVIEW_CHAR_LIMIT,
    chunk_text,
    clean_text,
    detect_language_safe,
    load_text_from_file,
    sample_texts,
    summarize_text,
)


st.set_page_config(
    page_title="AI vs Human Detector",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded",
)


@st.cache_resource(show_spinner=False)
def cached_baseline():
    return load_baseline_model()


@st.cache_resource(show_spinner=False)
def cached_hf(model_name: str):
    return load_hf_model(model_name)


def run_inference(text: str, model_name: str, use_hf: bool):
    chunks = chunk_text(text, max_chars=1200)
    start = time.time()
    if use_hf:
        model = cached_hf(model_name)
    else:
        model = cached_baseline()
    probs = [predict_proba(model, chunk, use_hf=use_hf) for chunk in chunks]
    avg_ai = sum(p["ai"] for p in probs) / len(probs)
    avg_human = sum(p["human"] for p in probs) / len(probs)
    elapsed_ms = int((time.time() - start) * 1000)
    return {"ai": avg_ai, "human": avg_human, "elapsed_ms": elapsed_ms, "chunks": len(chunks)}


def render_gauges(prob_ai: float, prob_human: float):
    fig, ax = plt.subplots(figsize=(4, 2))
    ax.barh(["AI", "Human"], [prob_ai * 100, prob_human * 100], color=["#e4572e", "#4b9cd3"])
    ax.set_xlim(0, 100)
    ax.set_xlabel("Probability (%)")
    for i, v in enumerate([prob_ai, prob_human]):
        ax.text(v * 100 + 1, i, f"{v*100:.1f}%", va="center")
    st.pyplot(fig, use_container_width=True)

    fig2, ax2 = plt.subplots(figsize=(3, 3), subplot_kw=dict(aspect="equal"))
    values = [prob_ai, prob_human]
    labels = ["AI", "Human"]
    colors = ["#e4572e", "#4b9cd3"]
    wedges, _ = ax2.pie(values, labels=labels, autopct="%1.1f%%", startangle=90, colors=colors)
    centre_circle = plt.Circle((0, 0), 0.55, fc="white")
    fig2.gca().add_artist(centre_circle)
    st.pyplot(fig2, use_container_width=True)


def main():
    st.title("AI vs Human æ–‡ç« åµæ¸¬å™¨")
    st.caption("TF-IDF + Logistic Regression baselineï¼Œä¸¦å¯åˆ‡æ› Hugging Face transformers æ¨¡å‹ã€‚")

    st.sidebar.header("è¼¸å…¥")
    input_mode = st.sidebar.radio("é¸æ“‡è¼¸å…¥æ–¹å¼", ["æ–‡å­—è¼¸å…¥", "æª”æ¡ˆä¸Šå‚³", "ç¯„ä¾‹æ¸¬è©¦"])

    raw_text = ""
    uploaded_preview = ""
    if input_mode == "æ–‡å­—è¼¸å…¥":
        raw_text = st.text_area("è¼¸å…¥æ–‡æœ¬ï¼ˆæ”¯æ´ä¸­æ–‡/è‹±æ–‡ï¼‰", height=220, placeholder="è²¼ä¸Šæƒ³æª¢æ¸¬çš„æ–‡ç« ...")
    elif input_mode == "æª”æ¡ˆä¸Šå‚³":
        file = st.file_uploader("ä¸Šå‚³ .txt æˆ– .docx", type=["txt", "docx"])
        if file:
            raw_text = load_text_from_file(file)
            uploaded_preview = summarize_text(raw_text, PREVIEW_CHAR_LIMIT)
    else:
        sample_choice = st.sidebar.selectbox("é¸æ“‡ç¯„ä¾‹", list(sample_texts.keys()))
        raw_text = sample_texts[sample_choice]
        st.info(f"å·²è¼‰å…¥ç¯„ä¾‹ï¼š{sample_choice}")

    st.sidebar.header("æ¨¡å‹é¸æ“‡")
    model_source = st.sidebar.radio("æ¨¡å‹é¡å‹", ["Baseline (TF-IDF + LR)", "Transformers"])
    use_hf = model_source.startswith("Transformers")
    hf_models = get_available_hf_models()
    hf_model_name = st.sidebar.selectbox("Transformers æ¨¡å‹ (é¦–æ¬¡è¼‰å…¥éœ€ç­‰å¾…ä¸‹è¼‰)", hf_models, index=0, disabled=not use_hf)
    max_len = st.sidebar.slider("é•·æ–‡æˆªæ–· (chars)", min_value=500, max_value=4000, value=2000, step=100)
    if use_hf:
        st.sidebar.info("Transformers é¦–æ¬¡è¼‰å…¥éœ€ä¸‹è¼‰æ¨¡å‹ï¼Œè«‹è€å¿ƒç­‰å€™ï¼›è‹¥ç’°å¢ƒå—é™è«‹æ”¹ç”¨ Baselineã€‚")

    st.sidebar.markdown("---")
    st.sidebar.caption("æª”æ¡ˆå¤§å°é™åˆ¶ 2 MBï¼›é•·æ–‡æœƒæˆªæ–·/åˆ†æ®µå¹³å‡ã€‚")

    if uploaded_preview:
        st.sidebar.subheader("æª”æ¡ˆé è¦½")
        st.sidebar.write(uploaded_preview)

    if st.button("ç«‹å³åµæ¸¬", type="primary") or (input_mode == "ç¯„ä¾‹æ¸¬è©¦" and raw_text):
        if not raw_text:
            st.warning("è«‹å…ˆè¼¸å…¥æˆ–ä¸Šå‚³æ–‡æœ¬")
            return
        lang = detect_language_safe(raw_text)
        if lang not in {"zh", "en"}:
            st.error("ç›®å‰åƒ…æ”¯æ´ä¸­æ–‡/è‹±æ–‡ï¼Œè«‹æä¾›ç›¸æ‡‰æ–‡æœ¬ã€‚")
            return
        cleaned = clean_text(raw_text, max_chars=max_len)
        with st.spinner("æ¨¡å‹æ¨è«–ä¸­..."):
            result = run_inference(cleaned, hf_model_name, use_hf=use_hf)

        st.success("å®Œæˆï¼")
        col1, col2 = st.columns([1.2, 1])
        with col1:
            st.metric("AI æ©Ÿç‡", f"{result['ai']*100:.2f}%")
            st.metric("Human æ©Ÿç‡", f"{result['human']*100:.2f}%")
            st.caption(f"æ¨¡å‹ï¼š{'Transformers' if use_hf else 'Baseline'} | {hf_model_name if use_hf else 'TF-IDF + LR'}")
            st.caption(f"è€—æ™‚ï¼š{result['elapsed_ms']} ms | åˆ†æ®µæ•¸ï¼š{result['chunks']}")
        with col2:
            render_gauges(result["ai"], result["human"])

        st.subheader("è©³ç´°è³‡è¨Š")
        st.write(
            f"**èªè¨€**ï¼š{lang} ï½œ **å­—æ•¸**ï¼š{len(cleaned)} ï½œ **æ¨¡å‹**ï¼š{'HF/' + hf_model_name if use_hf else 'TF-IDF + LR'}"
        )
        st.text_area("è¼¸å…¥æ–‡æœ¬ (æ¸…ç†å¾Œ)", cleaned, height=200)

        if use_hf:
            st.info("Transformers è¼¸å‡ºç‚ºæ¯æ®µçš„ logits/æ©Ÿç‡å¹³å‡ã€‚è‹¥æ¨¡å‹æ¨™ç±¤èˆ‡ AI/Human ä¸ç¬¦ï¼Œå·²å˜—è©¦æ˜ å°„ã€‚")


if __name__ == "__main__":
    main()
